def dqn(weight_socioeconIN,weight_infectedIN,weight_timeIN,VaccineIN,preTrainedIN,onlyExploitIN):
  # -*- coding: utf-8 -*-
  """Covid-19 DQN Simple Stratified with Vaccine v3.ipynb
  
  Automatically generated by Colaboratory.
  
  Original file is located at
      https://colab.research.google.com/drive/1buIiFwyop7nMikyZlxhQuJ7WwWgc9LyN
  """
  
  #STRATIFIED VERSION
  #ELU invece che ReLU, terminate reward con win/loose, few episodes, done in memoria (come usarlo? reward molto diversi per done e not done)
  import torch
  import torch.nn as nn
  import numpy as np
  import time
  import math
  import random
  from numpy import exp
  import matplotlib.pyplot as plt
  from scipy.integrate import odeint
  from scipy.optimize import minimize
  
  # ------------------------------------------------------------------------------------------
  import pathlib
  import os
  import shutil
  my_path = os.path.dirname(os.path.abspath(__file__))
  model_save_name = 'SimpleStratifiedVaccineWeights2.pth'
  path = os.path.join(my_path,model_save_name)
  images_dir = os.path.join(my_path,'static\\images\\Simple Stratified 3 Layers Vaccine')
  
  if os.path.isdir(images_dir):
      shutil.rmtree(images_dir)
      os.mkdir(images_dir)
  else:
      os.mkdir(images_dir)
  # ------------------------------------------------------------------------------------------
  
  MAX_RUN = 52
  FRESH_TIME = 0.1
  
    
  #torch.cuda.get_device_name(0)
  def odefun(x_0, t,beta0):
    """
    Time derivative of the state vector.
  
      * x is the state vector (array_like)
      * t is time (scalar)
      * beta0 is the passed parameter 
  
    """
    beta_uu,beta_um,beta_uo,beta_mu,beta_mm,beta_mo,beta_ou,beta_om,beta_oo = beta0
    #b3_u, b3_m, b3_o = b3  #b3 variable depending on action
    #v_u, v_m, v_o = v      #vaccine variable depending on action
  
  
    x_u=x_0[0:6]
    x_m=x_0[6:12]
    x_o=x_0[12:18]
    s_u, i1_u, i2_u, r1_u, r2_u,r3_u = x_u   #unpack the state
    s_m, i1_m, i2_m, r1_m, r2_m,r3_u = x_m 
    s_o, i1_o, i2_o, r1_o, r2_o,r3_u = x_o
  
    #r3_tot=r3_u+r3_m...=1-s_u-s_m-s_o-i1_u-..... $Da verificare
    ds_u=-s_u*(beta_uu*(i1_u+i2_u)+beta_um*(i1_m+i2_m)+beta_uo*(i1_o+i2_o))+d1*r2_u
    di1_u=s_u*(beta_uu*(i1_u+i2_u)+beta_um*(i1_m+i2_m)+beta_uo*(i1_o+i2_o))-(b1+b2)*i1_u
    di2_u=b2*i1_u-c1*i2_u-b3_u*i2_u
    dr1_u=b1*i1_u-(c2+c3_u)*r1_u
    dr2_u=c1*i2_u+c2*r1_u-d1*r2_u
    dr3_u=c3_u*r1_u
  
    ds_m=-s_m*(beta_mu*(i1_u+i2_u)+beta_mm*(i1_m+i2_m)+beta_mo*(i1_o+i2_o))+d1*r2_m
    di1_m=s_m*(beta_mu*(i1_u+i2_u)+beta_mm*(i1_m+i2_m)+beta_mo*(i1_o+i2_o))-(b1+b2)*i1_m
    di2_m=b2*i1_m-c1*i2_m-b3_m*i2_m
    dr1_m=b1*i1_m-(c2+c3_m)*r1_m
    dr2_m=c1*i2_m+c2*r1_m-d1*r2_m
    dr3_m=c3_m*r1_m
  
    ds_o=-s_o*(beta_ou*(i1_u+i2_u)+beta_om*(i1_m+i2_m)+beta_oo*(i1_o+i2_o))+d1*r2_o
    di1_o=s_o*(beta_ou*(i1_u+i2_u)+beta_om*(i1_m+i2_m)+beta_oo*(i1_o+i2_o))-(b1+b2)*i1_o
    di2_o=b2*i1_o-c1*i2_o-b3_o*i2_o
    dr1_o=b1*i1_o-(c2+c3_o)*r1_o
    dr2_o=c1*i2_o+c2*r1_o-d1*r2_o
    dr3_o=c3_o*r1_o
  
    #state equations
    #ds=-beta0*s*(i1+i2)+d1*r2
    #di1=beta0*s*(i1+i2)-(b1+b2)*i1
    #di2=b2*i1-c1*i2-b_3*i2
    #dr2=(c1+c2*b1/(b2-b1))*i2-d1*r2
  
    return ds_u, di1_u, di2_u, dr1_u, dr2_u, dr3_u, ds_m, di1_m, di2_m, dr1_m, dr2_m, dr3_m, ds_o, di1_o, di2_o, dr1_o, dr2_o, dr3_o
  
  def spread_virus(t, beta0, x):  #funzione di simulazione pandemia
    #G = lambda x, t: F(x, t, beta0,b_3)
    x_0=x[0:18]
    G = lambda x, t: odefun(x, t, beta0)#non considero b_3 (tampone)
    s_u, i1_u, i2_u, r1_u, r2_u, r3_u, s_m, i1_m, i2_m, r1_m, r2_m, r3_m, s_o, i1_o, i2_o, r1_o, r2_o, r3_o = odeint(G, x_0, t).transpose() #solve ode system
          # per risultati cumulativi:
          # for i in range (len(s)):
          #     i1_tot += beta0*s(i)*(i1(i)+i2(i))
          #     guariti_tot +=(c1+c2*b1/b2)*i2(i)
      #r1_path = (1/(b2/b1-1))*i2_path     # cumulative cases
      #r3_path = c3*r1_path
    return  s_u, i1_u, i2_u, r1_u, r2_u, r3_u, s_m, i1_m, i2_m, r1_m, r2_m, r3_m, s_o, i1_o, i2_o, r1_o, r2_o, r3_o
  
  def odefun_vax(x_0, t,beta0,u):
    """
    Time derivative of the state vector.
  
      * x is the state vector (array_like)
      * t is time (scalar)
      * beta0 is the passed parameter 
  
    """
    beta_uu,beta_um,beta_uo,beta_mu,beta_mm,beta_mo,beta_ou,beta_om,beta_oo = beta0
    #b3_u, b3_m, b3_o = b3  #b3 variable depending on action
    #v_u, v_m, v_o = v      #vaccine variable depending on action
  
  
    x_u=x_0[0:6]
    x_m=x_0[6:12]
    x_o=x_0[12:18]
    
    
    s_u, i1_u, i2_u, r1_u, r2_u, r3_u = x_u   #unpack the state
    s_m, i1_m, i2_m, r1_m, r2_m, r3_m = x_m 
    s_o, i1_o, i2_o, r1_o, r2_o, r3_o = x_o
    u_1, u_2, u_3= u
  
    #r3_tot=r3_u+r3_m...=1-s_u-s_m-s_o-i1_u-..... $Da verificare
    
    ds_u=-s_u*(beta_uu*(i1_u+i2_u)+beta_um*(i1_m+i2_m)+beta_uo*(i1_o+i2_o))+d1*r2_u-u_1
    di1_u=s_u*(beta_uu*(i1_u+i2_u)+beta_um*(i1_m+i2_m)+beta_uo*(i1_o+i2_o))-(b1+b2)*i1_u
    di2_u=b2*i1_u-c1*i2_u-b3_u*i2_u
    dr1_u=b1*i1_u-(c2+c3_u)*r1_u
    dr2_u=c1*i2_u+c2*r1_u-d1*r2_u
    dr3_u=c3_u*r1_u
  
    ds_m=-s_m*(beta_mu*(i1_u+i2_u)+beta_mm*(i1_m+i2_m)+beta_mo*(i1_o+i2_o))+d1*r2_m-u_2
    di1_m=s_m*(beta_mu*(i1_u+i2_u)+beta_mm*(i1_m+i2_m)+beta_mo*(i1_o+i2_o))-(b1+b2)*i1_m
    di2_m=b2*i1_m-c1*i2_m-b3_m*i2_m
    dr1_m=b1*i1_m-(c2+c3_m)*r1_m
    dr2_m=c1*i2_m+c2*r1_m-d1*r2_m
    dr3_m=c3_m*r1_m
  
    ds_o=-s_o*(beta_ou*(i1_u+i2_u)+beta_om*(i1_m+i2_m)+beta_oo*(i1_o+i2_o))+d1*r2_o-u_3
    di1_o=s_o*(beta_ou*(i1_u+i2_u)+beta_om*(i1_m+i2_m)+beta_oo*(i1_o+i2_o))-(b1+b2)*i1_o
    di2_o=b2*i1_o-c1*i2_o-b3_o*i2_o
    dr1_o=b1*i1_o-(c2+c3_o)*r1_o
    dr2_o=c1*i2_o+c2*r1_o-d1*r2_o
    dr3_o=c3_o*r1_o
  
    dv=u_1+u_2+u_3
  
    #state equations
    #ds=-beta0*s*(i1+i2)+d1*r2
    #di1=beta0*s*(i1+i2)-(b1+b2)*i1
    #di2=b2*i1-c1*i2-b_3*i2
    #dr2=(c1+c2*b1/(b2-b1))*i2-d1*r2
  
    return ds_u, di1_u, di2_u, dr1_u, dr2_u, dr3_u, ds_m, di1_m, di2_m, dr1_m, dr2_m, dr3_m, ds_o, di1_o, di2_o, dr1_o, dr2_o, dr3_o, dv
  
  
  
  
  def spread_virus_vax(t, beta0, x):  #funzione di simulazione pandemia
    #G = lambda x, t: F(x, t, beta0,b_3)
    u=x[19:22]
    x_0=x[0:19]
    
    G_vax = lambda x, t: odefun_vax(x, t, beta0,u)#non considero b_3 (tampone)
    s_u, i1_u, i2_u, r1_u, r2_u, r3_u, s_m, i1_m, i2_m, r1_m, r2_m, r3_m, s_o, i1_o, i2_o, r1_o, r2_o, r3_o, v= odeint(G_vax, x_0, t).transpose() #solve ode system
          # per risultati cumulativi:
          # for i in range (len(s)):
          #     i1_tot += beta0*s(i)*(i1(i)+i2(i))
          #     guariti_tot +=(c1+c2*b1/b2)*i2(i)
      #r1_path = (1/(b2/b1-1))*i2_path     # cumulative cases
      #r3_path = c3*r1_path
    #v=(u1+u2+u3)*t_span+v_0
    return s_u, i1_u, i2_u, r1_u, r2_u,r3_u, s_m, i1_m, i2_m, r1_m, r2_m,r3_m, s_o, i1_o, i2_o, r1_o, r2_o,r3_o,v
  
  def objFcn(u, t_vec, X0, beta0): 
      #usati valori random giusto per vedere il funzionamento del codice
      beta_uu,beta_um,beta_uo,beta_mu,beta_mm,beta_mo,beta_ou,beta_om,beta_oo = beta0
      
      X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15,X16,X17,X18,X19 = odeint(lambda x,t: odefun_vax(x,t,beta0,u),X0,t_vec).transpose()
      flux_under = np.multiply(X1,(beta_uu * (X2 + X3) + beta_um * (X7 + X8) + beta_uo * (X12 + X13)))
      flux_middle = np.multiply(X6,(beta_mm * (X7 + X8) + beta_mu * (X2 + X3) + beta_mo * (X12 + X13)))
      flux_senior = np.multiply(X11,(beta_oo * (X12 + X13) + beta_om * (X7 + X8) + beta_ou * (X2 + X3)))
      death = X6+X12+X18
      cost = sum(flux_under + flux_middle + flux_senior + death)
      return cost   
  
  def constrain1(u):
    u1,u2,u3=u
    omega=0.001
    return -(u1+u2+u3)+omega
  
  def update_env(INFECTED, STEP, ACTION, REWARD):#outuput stream
      print("Week %d, Action: %s, %.5f Infected, Reward %.2f" % (STEP, ACTION, INFECTED, REWARD))
      time.sleep(FRESH_TIME)
  
  class Environment(object):
      def __init__(self,perc,weights,Vaccine,initial_state,initial_vaccination):
          super(Environment, self).__init__()
          self.action_space = ['all_open', 'OS_HB' ,'OS_LB', 'HS_OB', 'HS_HB', 'HS_LB', 'LS_OB','LS_HB', 'all_closed']  # possible actions 
          self.n_actions = len(self.action_space) 
          perc_u,perc_o,perc_m=perc
          #percentuali di popolazione (https://www.tuttitalia.it/statistiche/popolazione-eta-sesso-stato-civile-2019/)
          self.perc_u=perc_u #percentage of under 30 y.o italian people
          self.perc_o=perc_o #percentage of over 65 y.o italian people
          self.perc_m=1-self.perc_u-self.perc_o #percentage of over 30 and under 65 y.o italian people
          #This proportion will be used in assesing the number of infected people and to plot results

          self.Vaccine=Vaccine
          self.weight_socioecon, self.weight_infect, self.weight_death=weights

          self.i1_u_0 = random.uniform(5e-2, 2e-1)
          self.i1_m_0 = random.uniform(5e-2, 2e-1)
          self.i1_o_0 = random.uniform(5e-2, 2e-1)

          self.u = initial_vaccination

          self.x_u=initial_state[0:6]
          self.x_m=initial_state[6:12]
          self.x_o=initial_state[12:18]

          self.state=np.concatenate((self.x_u,self.x_m),axis=None)
          self.state=np.concatenate((self.state,self.x_o),axis=None)        
          self.state=np.concatenate((self.state,0),axis=None) #add v info to the state
          self.state=np.concatenate((self.state,self.u),axis=None) #add vaccination to the state
          self.state=np.concatenate((self.state,0),axis=None) #add step info to the state
          self.state=np.concatenate((self.state,0),axis=None) #add done info to the state

          
          self.beta_0=(2.1058*self.perc_u, 1.0394*self.perc_u, 0.6006*self.perc_u, 1.0886*self.perc_m, 1.6950*self.perc_m, 0.7933*self.perc_m, 0.4711*self.perc_o, 1.0956*self.perc_o, 1.2*self.perc_o)
          self.beta0_max=np.mean(self.beta_0)
          self.beta=self.beta_0
        

          self.last_action = None
          t_stop=5
          self.t_prev=2*t_stop
          self.t_vec=np.linspace(0,self.t_prev-1,self.t_prev)
          self.c3_u = 0
          self.c3_m = 0.0063*self.perc_m
          self.c3_o = 0.1423*self.perc_o
          
          self.min_infected=0.00001 #1 person every 100 thousand people
          self.max_infected=0.8 
          self.max_time=100
          self.rew_cum=0
          self.action_list='Start'
          self.history_u=np.array([self.x_u[0],
                                self.x_u[1],
                                self.x_u[2],
                                self.x_u[3],
                                self.x_u[4],
                                self.x_u[5]])
            
          self.history_m=np.array([self.x_m[0],
                                  self.x_m[1],
                                  self.x_m[2],
                                  self.x_m[3],
                                  self.x_m[4],
                                  self.x_m[5]])
              
          self.history_o=np.array([self.x_o[0],
                                  self.x_o[1],
                                  self.x_o[2],
                                  self.x_o[3],
                                  self.x_o[4],
                                  self.x_o[5]])
          self.history_prev_u=self.history_u
          self.history_prev_m=self.history_m
          self.history_prev_o=self.history_o
          u10 = 1e-5
          u20 = 1e-5
          u30 = 1e-5
          self.u0 = np.array([u10,u20,u30])

      def reset(self):
          self.i1_u_0 = random.uniform(5e-2, 3e-1)
          self.i1_m_0 = random.uniform(5e-2, 3e-1)
          self.i1_o_0 = random.uniform(5e-2, 3e-1)

          self.x_u = (1.-self.i1_u_0,self.i1_u_0,0,0,0,0)
          self.x_m = (1.-self.i1_m_0,self.i1_m_0,0,0,0,0)
          self.x_o = (1.-self.i1_o_0,self.i1_o_0,0,0,0,0)
          self.u = (0.8e-8,1.4e-4,1e-5)
          self.state=np.concatenate((self.x_u,self.x_m),axis=None)
          self.state=np.concatenate((self.state,self.x_o),axis=None)        
          self.state=np.concatenate((self.state,0),axis=None) #add v info to the state
          self.state=np.concatenate((self.state,self.u),axis=None)
          self.state=np.concatenate((self.state,0),axis=None) #add step info to the state
          self.state=np.concatenate((self.state,0),axis=None) #add done info to the state

          
          self.beta_0=(2.1058*self.perc_u, 1.0394*self.perc_u, 0.6006*self.perc_u, 1.0886*self.perc_m, 1.6950*self.perc_m, 0.7933*self.perc_m, 0.4711*self.perc_o, 1.0956*self.perc_o, 1.2*self.perc_o)
          self.beta=self.beta_0
          
          
          self.rew_cum=0
          #self.previous_infected=self.x_u[1]+self.x_m[1]+self.x_o[1] 
          self.action_list='Start'

          self.history_u=np.array([self.x_u[0],
                                  self.x_u[1],
                                  self.x_u[2],
                                  self.x_u[3],
                                  self.x_u[4],
                                  self.x_u[5]])
              
          self.history_m=np.array([self.x_m[0],
                                  self.x_m[1],
                                  self.x_m[2],
                                  self.x_m[3],
                                  self.x_m[4],
                                  self.x_m[5]])
              
          self.history_o=np.array([self.x_o[0],
                                  self.x_o[1],
                                  self.x_o[2],
                                  self.x_o[3],
                                  self.x_o[4],
                                  self.x_o[5]])
          self.history_prev_u=self.history_u
          self.history_prev_m=self.history_m
          self.history_prev_o=self.history_o
          u10 = 1e-5
          u20 = 1e-5
          u30 = 1e-5
          self.u0 = np.array([u10,u20,u30])
          
          return self.state
      
      def instant_reward(self, delta_infect, step,beta0):
        weight_infected=-50*self.weight_infect
        weight_socioecon=12/500*self.weight_socioecon
        weight_death=-20*self.weight_death
        beta_uu,beta_um,beta_uo,beta_mu,beta_mm,beta_mo,beta_ou,beta_om,beta_oo = beta0
        beta_mean=np.mean(beta0)
        
        flux_under =  np.multiply(self.current_s_u,(beta_uu * (self.current_i1_u + self.current_i2_u) + beta_um * (self.current_i1_m + self.current_i2_m) + beta_uo * (self.current_i1_o + self.current_i2_o)))
        flux_middle = np.multiply(self.current_s_m,(beta_mm * (self.current_i1_m + self.current_i2_m) + beta_mu * (self.current_i1_u + self.current_i2_u) + beta_mo * (self.current_i1_o + self.current_i2_o)))
        flux_senior = np.multiply(self.current_s_o,(beta_oo * (self.current_i1_o + self.current_i2_o) + beta_om * (self.current_i1_m + self.current_i2_m) + beta_ou * (self.current_i1_u + self.current_i2_u)))
        #print(f"abbiamo {X1},{X2},{X3},{X4},{X5},{X6},{X7},{X8},{X9},{X10},{X11},{X12},{X13},{X14},{X15},{X16},{X17},{X18}")
        #print(f"quindi {flux_under}, {flux_middle}, {flux_senior}")
        #print(f"dim = {X1.ndim}")
        if flux_under.ndim>0:
          rew_infected= sum(flux_under + flux_middle + flux_senior)*weight_infected
          rew_death = sum(self.current_r3_u+self.current_r3_m+self.current_r3_o)*weight_death
        else:
          rew_infected= (flux_under + flux_middle + flux_senior)*weight_infected
          rew_death = (self.current_r3_u+self.current_r3_m+self.current_r3_o)*weight_death
        
        tot_r1=self.current_r1_u[-1]+self.current_r1_m[-1]+self.current_r1_o[-1]
        rew_socioecon=(beta_mean-self.beta0_max)*math.exp((1-self.infected+tot_r1)*10)*weight_socioecon
        reward = rew_infected + rew_death +rew_socioecon
        #print(f"rew_inf is {rew_infected}, rew_socioecon is {rew_socioecon}, rew_death is {rew_death}")
        self.rew_cum=self.rew_cum+reward
        return reward
        
      
      def terminate_reward(self):
        if self.infected<=self.min_infected:
          if np.absolute(self.rew_cum)>0.00001:
            reward=-1/self.rew_cum
          else:
            reward=100
            
        else:
            reward = self.rew_cum #The whole episode reward is different from each step reward
        
        return reward
        
    
    
    
      def step(self, action, step, show):
        #s suscptible
        #i1 infected
        #i2 undetected
        #r1 hospitalized
        #r2 recovered
        #check what would happen if no action are taken (same parameters as last cycle)
        if self.Vaccine:
          self.prev_s_u, self.prev_i1_u, self.prev_i2_u, self.prev_r1_u, self.prev_r2_u, self.prev_r3_u, self.prev_s_m, self.prev_i1_m, self.prev_i2_m, self.prev_r1_m, self.prev_r2_m, self.prev_r3_m, self.prev_s_o, self.prev_i1_o, self.prev_i2_o, self.prev_r1_o, self.prev_r2_o, self.prev_r3_o, self.prev_v = spread_virus_vax(self.t_vec+step*self.t_prev, self.beta, self.state) #prevision of the trend if no action is taken
        else:
          self.prev_s_u, self.prev_i1_u, self.prev_i2_u, self.prev_r1_u, self.prev_r2_u, self.prev_r3_u, self.prev_s_m, self.prev_i1_m, self.prev_i2_m, self.prev_r1_m, self.prev_r2_m, self.prev_r3_m, self.prev_s_o, self.prev_i1_o, self.prev_i2_o, self.prev_r1_o, self.prev_r2_o, self.prev_r3_o = spread_virus(self.t_vec+step*self.t_prev, self.beta, self.state) #prevision of the trend if no action is taken
          self.prev_v=0
        
        #self.prev_infected=self.prev_infected[-1]+self.prev_undetected[-1]
        self.prev_infected = self.perc_u*(self.prev_i1_u[-1] + self.prev_i2_u[-1]+self.prev_r1_u[-1]) + self.perc_m*(self.prev_i1_m[-1] + self.prev_i2_m[-1]+self.prev_r1_m[-1]) + self.perc_o*(self.prev_i1_o[-1] + self.prev_i2_o[-1]+self.prev_r1_o[-1]) 
        self.action_list=np.concatenate((self.action_list,self.action_space[action]),axis=None)
        
        
        if (self.action_space[action] == 'all_open'): # a seconda dell'azione bisogna cambiare il parametro R0 e volendo B_3
          self.beta=[x*(80+20)/100 for x in self.beta_0] #non si può moltiplicare tuple con un float
        elif (self.action_space[action] == 'OS_HB'):
          self.beta=[x*(80+10)/100 for x in self.beta_0]
        elif (self.action_space[action] == 'OS_LB'):
          self.beta=[x*(80+0)/100 for x in self.beta_0]
        elif (self.action_space[action] == 'HS_OB'):
          self.beta=[x*(40+20)/100 for x in self.beta_0]
        elif (self.action_space[action] == 'HS_HB'):
          self.beta=[x*(40+10)/100 for x in self.beta_0]
        elif (self.action_space[action] == 'HS_LB'):
          self.beta=[x*(40+0)/100 for x in self.beta_0]
        elif (self.action_space[action] == 'LS_OB'):
          self.beta=[x*(0+20)/100 for x in self.beta_0]
        elif (self.action_space[action] == 'LS_HB'):
          self.beta=[x*(0+10)/100 for x in self.beta_0]
        else: # (action == 'all_closed'):
          self.beta=[x*(0+0)/100 for x in self.beta_0] 
        #self.beta=self.w_school*self.beta_scuole + self.w_bus*self.beta_bus
        if self.Vaccine:
          self.u1 = minimize(lambda u: objFcn(u,self.t_vec, self.state[0:19], self.beta),self.u0,method='SLSQP',bounds=bnds,constraints=cons)
          self.u0=self.u1.x
        #print(f"OPTIMIZATION RESULT IS ooooooooooooooooo {self.u0}")
        
        if self.Vaccine:
          self.current_s_u, self.current_i1_u, self.current_i2_u, self.current_r1_u, self.current_r2_u,self.current_r3_u, self.current_s_m, self.current_i1_m, self.current_i2_m, self.current_r1_m, self.current_r2_m,self.current_r3_m, self.current_s_o, self.current_i1_o, self.current_i2_o, self.current_r1_o, self.current_r2_o,self.current_r3_o, self.current_v = spread_virus_vax(self.t_vec+step*self.t_prev, self.beta, self.state) #solve_path
        else:
          self.current_s_u, self.current_i1_u, self.current_i2_u, self.current_r1_u, self.current_r2_u,self.current_r3_u, self.current_s_m, self.current_i1_m, self.current_i2_m, self.current_r1_m, self.current_r2_m,self.current_r3_m, self.current_s_o, self.current_i1_o, self.current_i2_o, self.current_r1_o, self.current_r2_o, self.current_r3_o = spread_virus(self.t_vec+step*self.t_prev, self.beta, self.state) #solve_path
          self.current_v=0
  
        #self.current_susceptible, self.current_infected, self.current_undetected, self.current_recovered= spread_virus(self.t_vec, self.beta, self.state) #solve_path
        
        self.infected = self.perc_u*(self.current_i1_u[-1] + self.current_i2_u[-1]+self.current_r1_u[-1]) + self.perc_m*(self.current_i1_m[-1] + self.current_i2_m[-1] + self.current_r1_m[-1]) + self.perc_o*(self.current_i1_o[-1] + self.current_i2_o[-1]++self.current_r1_o[-1]) 
        #self.infected=self.current_infected[-1]+self.current_undetected[-1] #total infected 
        
        self.delta_infected =  self.prev_infected-self.infected #check if the prevision is better than the actual infect state, used for reward
        
    
        self.x_u  = (self.current_s_u[-1], self.current_i1_u[-1], self.current_i2_u[-1], self.current_r1_u[-1], self.current_r2_u[-1], self.current_r3_u[-1])
        self.x_m  = (self.current_s_m[-1], self.current_i1_m[-1], self.current_i2_m[-1], self.current_r1_m[-1], self.current_r2_m[-1], self.current_r3_m[-1])
        self.x_o  = (self.current_s_o[-1], self.current_i1_o[-1], self.current_i2_o[-1], self.current_r1_o[-1], self.current_r2_o[-1], self.current_r3_o[-1])
        #self.u= $$$
        
        reward = self.instant_reward(self.delta_infected, step,self.beta)
        
        if (step >= self.max_time) or (self.infected >= self.max_infected) or (self.infected<=self.min_infected) : #end episode condition
          done = True
          reward=self.terminate_reward()
          print("terimanate reward:", reward)
          print("rew_cum:", self.rew_cum)
        else:
          done = False
        if Vaccine:
          v=self.current_v[-1]
        else:
          v=0
        self.state=np.concatenate((self.x_u,self.x_m),axis=None)
        self.state=np.concatenate((self.state,self.x_o),axis=None)      
        self.state=np.concatenate((self.state,v),axis=None)
        self.state=np.concatenate((self.state,self.u),axis=None)
        self.state=np.concatenate((self.state,step),axis=None) #add step info to the state
        self.state=np.concatenate((self.state,done),axis=None) #add step info to the state
        #update_env(self.infected, step, self.action_space[action], reward)
        #time.sleep(0.02)#$ si può togliere?
  
        #save history to plot 
        if show:
          
          self.history_u=np.array([np.concatenate((self.history_u[0],self.current_s_u[1:-1]),axis=None),
                                np.concatenate((self.history_u[1],self.current_i1_u[1:-1]),axis=None),
                                np.concatenate((self.history_u[2],self.current_i2_u[1:-1]),axis=None),
                                np.concatenate((self.history_u[3],self.current_r1_u[1:-1]),axis=None),
                                np.concatenate((self.history_u[4],self.current_r2_u[1:-1]),axis=None),
                                np.concatenate((self.history_u[5],self.current_r3_u[1:-1]),axis=None)])
          self.history_m=np.array([np.concatenate((self.history_m[0],self.current_s_m[1:-1]),axis=None),
                                np.concatenate((self.history_m[1],self.current_i1_m[1:-1]),axis=None),
                                np.concatenate((self.history_m[2],self.current_i2_m[1:-1]),axis=None),
                                np.concatenate((self.history_m[3],self.current_r1_m[1:-1]),axis=None),
                                np.concatenate((self.history_m[4],self.current_r2_m[1:-1]),axis=None),
                                np.concatenate((self.history_m[5],self.current_r3_m[1:-1]),axis=None)])
          self.history_o=np.array([np.concatenate((self.history_o[0],self.current_s_o[1:-1]),axis=None),
                                np.concatenate((self.history_o[1],self.current_i1_o[1:-1]),axis=None),
                                np.concatenate((self.history_o[2],self.current_i2_o[1:-1]),axis=None),
                                np.concatenate((self.history_o[3],self.current_r1_o[1:-1]),axis=None),
                                np.concatenate((self.history_o[4],self.current_r2_o[1:-1]),axis=None),
                                np.concatenate((self.history_o[5],self.current_r3_o[1:-1]),axis=None)])
          
          
          self.history_prev_u=np.array([np.concatenate((self.history_prev_u[0],self.prev_s_u[1:-1]),axis=None),
                                np.concatenate((self.history_prev_u[1],self.prev_i1_u[1:-1]),axis=None),
                                np.concatenate((self.history_prev_u[2],self.prev_i2_u[1:-1]),axis=None),
                                np.concatenate((self.history_prev_u[3],self.prev_r1_u[1:-1]),axis=None),
                                np.concatenate((self.history_prev_u[4],self.prev_r2_u[1:-1]),axis=None),
                                np.concatenate((self.history_prev_u[5],self.prev_r3_m[1:-1]),axis=None)])
          self.history_prev_m=np.array([np.concatenate((self.history_prev_m[0],self.prev_s_m[1:-1]),axis=None),
                                np.concatenate((self.history_prev_m[1],self.prev_i1_m[1:-1]),axis=None),
                                np.concatenate((self.history_prev_m[2],self.prev_i2_m[1:-1]),axis=None),
                                np.concatenate((self.history_prev_m[3],self.prev_r1_m[1:-1]),axis=None),
                                np.concatenate((self.history_prev_m[4],self.prev_r2_m[1:-1]),axis=None),
                                np.concatenate((self.history_prev_m[5],self.prev_r3_m[1:-1]),axis=None)])
          self.history_prev_o=np.array([np.concatenate((self.history_prev_o[0],self.prev_s_o[1:-1]),axis=None),
                                np.concatenate((self.history_prev_o[1],self.prev_i1_o[1:-1]),axis=None),
                                np.concatenate((self.history_prev_o[2],self.prev_i2_o[1:-1]),axis=None),
                                np.concatenate((self.history_prev_o[3],self.prev_r1_o[1:-1]),axis=None),
                                np.concatenate((self.history_prev_o[4],self.prev_r2_o[1:-1]),axis=None),
                                np.concatenate((self.history_prev_o[5],self.prev_r3_o[1:-1]),axis=None)])
          
                                          
    
        return np.array(self.state), reward, done
  
  class DeepQNetwork(nn.Module):
    def __init__(self):
      super(DeepQNetwork, self).__init__()#inherit characteristic of DeepQNetwork class
      self.fc = nn.Sequential(
          #nn.Linear(2, 24),
          nn.Linear(24, 24), #20 input =state, 6 for each popoluation age + 1 for time +1 for done
          nn.ELU(),
          nn.Linear(24, 24),
          nn.ELU(),
          nn.Linear(24, 9) #7 output = q values: one for each action to be taken
      )
      self.mls = nn.MSELoss() #mean square error criterion, possible to chose square error or sum of square error
      self.opt = torch.optim.Adam(self.parameters(), lr=0.1)# first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments
      #more about adam at https://arxiv.org/abs/1412.6980
    
    def forward(self, inputs):
      return self.fc(inputs)
  
  class Explore():#explore function, given a time step, it returns a value from end to start which gradually decay to end which is not zero because always explore
    def __init__(self,start,end,decay):
      self.start=start
      self.end=end
      self.decay=decay
  
    def get_exploration_rate(self,current_step):
      return self.end + (self.start - self.end)*math.exp(-1.*current_step*self.decay)


  def plot_env(env,txt,episode,perc):
    perc_u,perc_m,perc_o=perc
    with plt.style.context('classic'):
  
      suc_u=(env.history_u[0])*perc_u
      prev_suc=(env.history_prev_u[0])*perc_u
      fig, ax = plt.subplots(figsize=(12, 4), num='classic')
      plt.plot(suc_u,'r',linewidth=2,label='under')
  
      suc_m=(env.history_m[0])*perc_m
      prev_suc+=(env.history_prev_m[0])*perc_m
      plt.plot(suc_m,'g',linewidth=2,label='middle')
  
      suc_o=(env.history_o[0])*perc_o
      prev_suc+=(env.history_prev_m[0])*perc_o
      plt.plot(suc_o,'b',linewidth=2,label='over')
  
      suc=suc_u+suc_m+suc_o
      plt.plot(suc,'k',linewidth=2,label='tot_suc')
      # Add labels to the plot
      style = dict(size=10, color='black')
      for i in range(1,len(env.action_list)):
        if env.action_list[i]!=env.action_list[i-1]:
          idx_suc=max(range(7,len(suc)), key=suc.__getitem__)
          index_action=1+8*(i-1)
          suc_action=suc[index_action-1]
          #print(f"AZIONE DIVERSA: passo {index_action} valore {suc_action}")
          if index_action>idx_suc or index_action==1:
            offset=20
          else:
            offset=-20
          #ax.text(7*i,suc[7*i], env.action_list[i], **style)
          
          ax.annotate(env.action_list[i], xy=(index_action, suc_action),xycoords='data',xytext=(index_action+offset, 0.05+0.02*math.sin(index_action)+suc_action),bbox=dict(boxstyle="round", alpha=0.2),arrowprops=dict(arrowstyle="fancy"))
      # Label the axes
      ax.set(title='SUSCEPTIBLE',
            ylabel='susceptible %')
      
      ax.set_xlabel(f"days\n\n")
  
      plt.ylim([0, max(max(suc),max(prev_suc))])
      ax=plt.plot(prev_suc,'k--',linewidth=1,label='previsions')
      plt.legend()
      
      fig.set_size_inches(20, 10, forward=True)

      # ------------------------------------------------------------------------------------------
      name = '2_'+str(episode)+'0suc.png'
      ptf = os.path.join(images_dir,name)
      plt.savefig(ptf)
      # ------------------------------------------------------------------------------------------
      plt.close()
      #plt.show()

    with plt.style.context('classic'):
  
      inf_u=(env.history_u[1]+env.history_u[2])*perc_u
      prev_inf=(env.history_prev_u[1]+env.history_prev_u[2])*perc_u
      fig, ax = plt.subplots(figsize=(12, 4), num='classic')
      plt.plot(inf_u,'r',linewidth=2,label='under')
  
      inf_m=(env.history_m[1]+env.history_m[2])*perc_m
      prev_inf+=(env.history_prev_m[1]+env.history_prev_m[2])*perc_m
      plt.plot(inf_m,'g',linewidth=2,label='middle')
  
      inf_o=(env.history_o[1]+env.history_o[2])*perc_o
      prev_inf+=(env.history_prev_m[1]+env.history_prev_m[2])*perc_o
      plt.plot(inf_o,'b',linewidth=2,label='over')
  
      inf=inf_u+inf_m+inf_o
      plt.plot(inf,'k',linewidth=2,label='tot_inf')
      # Add labels to the plot
      style = dict(size=10, color='black')
      for i in range(1,len(env.action_list)):
        if env.action_list[i]!=env.action_list[i-1]:
          idx_inf=max(range(7,len(inf)), key=inf.__getitem__)
          index_action=1+8*(i-1)
          inf_action=inf[index_action-1]
          #print(f"AZIONE DIVERSA: passo {index_action} valore {inf_action}")
          if index_action>idx_inf or index_action==1:
            offset=20
          else:
            offset=-20
          #ax.text(7*i,inf[7*i], env.action_list[i], **style)
          
          ax.annotate(env.action_list[i], xy=(index_action, inf_action),xycoords='data',xytext=(index_action+offset, 0.05+0.02*math.sin(index_action)+inf_action),bbox=dict(boxstyle="round", alpha=0.2),arrowprops=dict(arrowstyle="fancy"))
      # Label the axes
      ax.set(title='INFECTED',
            ylabel='infected %')
      
      ax.set_xlabel(f"days\n\n")
  
      plt.ylim([0, max(max(inf),max(prev_inf))])
      ax=plt.plot(prev_inf,'k--',linewidth=1,label='previsions')
      plt.legend()
      
      fig.set_size_inches(20, 10, forward=True)

      # ------------------------------------------------------------------------------------------
      name = '2_'+str(episode)+'1inf.png'
      ptf = os.path.join(images_dir,name)
      plt.savefig(ptf)
      # ------------------------------------------------------------------------------------------
      plt.close()
      #plt.show()

    with plt.style.context('classic'):
  
      hosp_u=(env.history_u[3])*perc_u
      prev_hosp=(env.history_prev_u[3])*perc_u
      fig, ax = plt.subplots(figsize=(12, 4), num='classic')
      plt.plot(hosp_u,'r',linewidth=2,label='under')
  
      hosp_m=(env.history_m[3])*perc_m
      prev_hosp+=(env.history_prev_m[3])*perc_m
      plt.plot(hosp_m,'g',linewidth=2,label='middle')
  
      hosp_o=(env.history_o[3])*perc_o
      prev_hosp+=(env.history_prev_m[3])*perc_o
      plt.plot(hosp_o,'b',linewidth=2,label='over')
  
      hosp=hosp_u+hosp_m+hosp_o
      plt.plot(hosp,'k',linewidth=2,label='tot_hosp')
      # Add labels to the plot
      style = dict(size=10, color='black')
      for i in range(1,len(env.action_list)):
        if env.action_list[i]!=env.action_list[i-1]:
          idx_hosp=max(range(7,len(hosp)), key=hosp.__getitem__)
          index_action=1+8*(i-1)
          hosp_action=hosp[index_action-1]
          #print(f"AZIONE DIVERSA: passo {index_action} valore {hosp_action}")
          if index_action>idx_hosp or index_action==1:
            offset=20
          else:
            offset=-20
          #ax.text(7*i,hosp[7*i], env.action_list[i], **style)
          
          ax.annotate(env.action_list[i], xy=(index_action, hosp_action),xycoords='data',xytext=(index_action+offset, 0.05+0.02*math.sin(index_action)+hosp_action),bbox=dict(boxstyle="round", alpha=0.2),arrowprops=dict(arrowstyle="fancy"))
      # Label the axes
      ax.set(title='HOSPEDALIZED',
            ylabel='hospedalized %')
      
      ax.set_xlabel(f"days\n\n")
  
      plt.ylim([0, max(max(hosp),max(prev_hosp))])
      ax=plt.plot(prev_hosp,'k--',linewidth=1,label='previsions')
      plt.legend()
      
      fig.set_size_inches(20, 10, forward=True)

      # ------------------------------------------------------------------------------------------
      name = '2_'+str(episode)+'2hos.png'
      ptf = os.path.join(images_dir,name)
      plt.savefig(ptf)
      # ------------------------------------------------------------------------------------------
      plt.close()
      #plt.show()

    with plt.style.context('classic'):
  
      rec_u=(env.history_u[4])*perc_u
      prev_rec=(env.history_prev_u[4])*perc_u
      fig, ax = plt.subplots(figsize=(12, 4), num='classic')
      plt.plot(rec_u,'r',linewidth=2,label='under')
  
      rec_m=(env.history_m[4])*perc_m
      prev_rec+=(env.history_prev_m[4])*perc_m
      plt.plot(rec_m,'g',linewidth=2,label='middle')
  
      rec_o=(env.history_o[4])*perc_o
      prev_rec+=(env.history_prev_m[4])*perc_o
      plt.plot(rec_o,'b',linewidth=2,label='over')
  
      rec=rec_u+rec_m+rec_o
      plt.plot(rec,'k',linewidth=2,label='tot_rec')
      # Add labels to the plot
      style = dict(size=10, color='black')
      for i in range(1,len(env.action_list)):
        if env.action_list[i]!=env.action_list[i-1]:
          idx_rec=max(range(7,len(rec)), key=rec.__getitem__)
          index_action=1+8*(i-1)
          rec_action=rec[index_action-1]
          #print(f"AZIONE DIVERSA: passo {index_action} valore {rec_action}")
          if index_action>idx_rec or index_action==1:
            offset=20
          else:
            offset=-20
          #ax.text(7*i,rec[7*i], env.action_list[i], **style)
          
          ax.annotate(env.action_list[i], xy=(index_action, rec_action),xycoords='data',xytext=(index_action+offset, 0.05+0.02*math.sin(index_action)+rec_action),bbox=dict(boxstyle="round", alpha=0.2),arrowprops=dict(arrowstyle="fancy"))
      # Label the axes
      ax.set(title='RECOVERED',
            ylabel='recovered %')
      
      ax.set_xlabel(f"days\n\n")
  
      plt.ylim([0, max(max(rec),max(prev_rec))])
      ax=plt.plot(prev_rec,'k--',linewidth=1,label='previsions')
      plt.legend()
      
      fig.set_size_inches(20, 10, forward=True)

      # ------------------------------------------------------------------------------------------
      name = '2_'+str(episode)+'3rec.png'
      ptf = os.path.join(images_dir,name)
      plt.savefig(ptf)
      plt.close()
      # ------------------------------------------------------------------------------------------

      #plt.show()

    with plt.style.context('classic'):
  
      dea_u=(env.history_u[5])*perc_u
      prev_dea=(env.history_prev_u[5])*perc_u
      fig, ax = plt.subplots(figsize=(12, 4), num='classic')
      plt.plot(dea_u,'r',linewidth=2,label='under')
  
      dea_m=(env.history_m[5])*perc_m
      prev_dea+=(env.history_prev_m[5])*perc_m
      plt.plot(dea_m,'g',linewidth=2,label='middle')
  
      dea_o=(env.history_o[5])*perc_o
      prev_dea+=(env.history_prev_m[5])*perc_o
      plt.plot(dea_o,'b',linewidth=2,label='over')
  
      dea=dea_u+dea_m+dea_o
      plt.plot(dea,'k',linewidth=2,label='tot_dea')
      # Add labels to the plot
      style = dict(size=10, color='black')
      for i in range(1,len(env.action_list)):
        if env.action_list[i]!=env.action_list[i-1]:
          idx_dea=max(range(7,len(dea)), key=dea.__getitem__)
          index_action=1+8*(i-1)
          dea_action=dea[index_action-1]
          #print(f"AZIONE DIVERSA: passo {index_action} valore {dea_action}")
          if index_action>idx_dea or index_action==1:
            offset=20
          else:
            offset=-20
          #ax.text(7*i,dea[7*i], env.action_list[i], **style)
          
          ax.annotate(env.action_list[i], xy=(index_action, dea_action),xycoords='data',xytext=(index_action+offset, 0.05+0.02*math.sin(index_action)+dea_action),bbox=dict(boxstyle="round", alpha=0.2),arrowprops=dict(arrowstyle="fancy"))
      # Label the axes
      ax.set(title='DEATH',
            ylabel='death %')
      
      ax.set_xlabel(f"days\n\n")
  
      plt.ylim([0, max(max(dea),max(prev_dea))])
      ax=plt.plot(prev_dea,'k--',linewidth=1,label='previsions')
      plt.legend()
      
      fig.set_size_inches(20, 10, forward=True)

      # ------------------------------------------------------------------------------------------
      name = '2_'+str(episode)+'4dea.png'
      ptf = os.path.join(images_dir,name)
      plt.savefig(ptf)
      plt.close()
      # ------------------------------------------------------------------------------------------
      
      #plt.show()

  #PARAMETER DEFINITION
  memory_size = 5000 #Maximum size of the memory
  #decline = 0.00006
  update_time = 50 #Set how often update the target network
  gamma = 0.9 #Discount factor: how much we weight the old q-value wrt to the new one
  b_size = 1600 #batch size: dimension of each set of experiences sent to the network to be updated
  memory = np.zeros((memory_size, 59)) # 32 = State(24) + Action(1) + Next State(24) + Reward(1) + Beta(9) + Done(1)
  
  if onlyExploitIN:
    EPISODES=2
    show_every=1
  else:
    EPISODES=2000
    show_every=1000
  
  perc_u=0.28 #percentage of under 30 y.o italian people
  perc_o=0.23 #percentage of over 65 y.o italian people
  perc_m=1-perc_u-perc_o #percentage of over 30 and under 65 y.o italian people
  perc=np.array([perc_u,perc_o,perc_m])
  
  t1=5          #incubation period
  t2=17         #disease time
  
  
  # $$ COEFFICENTI VARI DA RIVISIONARE, NON REGGE PIù L'IPOTESI tra C1 C2 C3 e b1 b2
  b1=0.3/t1     #detection rate, from i1 to r1
  b2=0.7/t1     #undetection rate, from i1 to i2
  
  
  c1=1/t2       #from i2 to r2, undetected healing ratio
  c2=0.8/t2     #from r1 to r2, detected healing ratio
  c3=1/t2-c2    #Fatality https://www.statista.com/statistics/1106372/coronavirus-death-rate-by-age-group-italy/
  
  c3_u = 0 #$devo fare una media pesata
  c3_m = 0.0063*perc_m
  c3_o = 0.1423*perc_o
  
  d1=0.001      #0.03  %back to susceptible (fine anticorpi)
  b_3=0.05      #simulation of tampon
  b3_u = b_3 
  b3_m = b_3 
  b3_o = b_3

  action_count=np.array([0,0,0,0,0,0,0,0,0])
  
  epsilon = 0.9 #random action to explore stuff
  maxeps=1
  mineps=0.01
  epsilon_decay_value=5e-4
  omega=0.001 #max total vaccine per week
  b=[0, omega] #each u limits
  
  bnds=(b, b, b)
  cons1={'type' : 'ineq', 'fun' : constrain1}  #constraint for optimization
  cons=[cons1]

  def run_deepQ(env, net, net2, onlyExploit_IN):
    convergence_episode=0
    loss_list=np.array([0,0])
  
    memory_count = 0
    learn_time = 0
  
    strategy=Explore(maxeps,mineps,epsilon_decay_value)
    show = False

    EPISODES_IN = EPISODES
    show_every_IN = show_every
    if onlyExploit_IN:
      Eps_greedy = 0.001 #use this to avoid exploration
      EPISODES_IN = 10
      show_every_IN = 1
    else:
      Eps_greedy = 100

    for episode in range(EPISODES):
      
      if show:
        #$super duper cool https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb#scrollTo=-aadvkYjQWoV
        #fig=plt.figure()
        txt="Stratified With Reward -self.infected/self.min_infected*1 and 3 layer Networks"
        plot_env(env,txt,episode,perc)
      if episode%show_every:
        show= False
      else:
        show= True
  
      if episode>=(EPISODES-1):
        plt.plot(loss_list)
        print(f"{action_count[0]} all_open, {action_count[1]} OS_HB, {action_count[2]} OS_LB, {action_count[3]} HS_OB, {action_count[4]} HS_HB, {action_count[5]} HS_LB, {action_count[6]} LS_OB, {action_count[7]} LS_HB, {action_count[8]} all_close")
      observation = env.reset()
      step = 0
      print(f" LEARNING PROCESS AT {100*episode/EPISODES}% ")
      while True:
        if (learn_time < 10):
          if (memory_count < memory_size):
            print(learn_time, memory_count)
          print(learn_time)

        
        if random.randint(0, 100) < Eps_greedy * strategy.get_exploration_rate(learn_time):

        # -------------------------------------------  
        #if random.randint(0, 100) < 0.001 * strategy.get_exploration_rate(learn_time): #use this to avoid exploration
        #if random.randint(0, 100) < 100 * strategy.get_exploration_rate(learn_time):
        # -------------------------------------------
          print("Explore")
          action = random.randint(0, env.n_actions-1)  # Randomly pick an action
          
        else:
          print("Exploit")
          out = net(torch.Tensor(observation).flatten()).detach()  # [left_rewards_total, right_reward_total]
          action_vec= out.data
          action = torch.argmax(action_vec).item()
          action_count[action]=  action_count[action] + 1
        observation_, reward, done = env.step(action, step, show)
        
  
        idx = memory_count % memory_size
        memory[idx][0:24] = observation
        memory[idx][24:25] = action
        memory[idx][25:49] = observation_
        memory[idx][49:50] = reward
        memory[idx][50:59] = env.beta
        memory[idx][59:60] = done
        
        memory_count +=1
        
        observation = observation_ 
  
        if (memory_count >= memory_size):  # Start to learn
          learn_time += 1 # Learn once
          if (learn_time % update_time == 0): # Sync two nets
            net2.load_state_dict(net.state_dict())
            print("Sync Two Net")
          else:
            rdp = random.randint(0, memory_size - b_size - 1)
            b_s = torch.Tensor(memory[rdp:rdp+b_size, 0:24])
            b_a = torch.Tensor(memory[rdp:rdp+b_size, 24:25]).long()
            b_s_ = torch.Tensor(memory[rdp:rdp+b_size, 25:49])
            b_r = torch.Tensor(memory[rdp:rdp+b_size, 49:50])
            b_b = torch.Tensor(memory[rdp:rdp+b_size, 50:59])      #batch consequent beta0
            b_d= torch.Tensor(memory[rdp:rdp+b_size, 59:60])  #done
  
            q = net(b_s).gather(1, b_a)       #take the q values as output of the policy net given the actual state as input and chose those of the action taken stored in b_a
            out_available=net2(b_s_).detach() #take best q_next as output of the target net given the next state as input corrisponding to the best action
            
            
            q_next = out_available.max(1)[0].reshape(b_size, 1) #best possible next_q value
            tq = b_r + gamma * q_next  #Bellman equation to compute the loss function
            loss = net.mls(q, tq) #loss function as MSE between q values and optimal function (rhs of bellman equation)
            loss_list=np.concatenate((loss_list,loss.detach().numpy()),axis=None)
            if loss.detach().numpy()<0.01 and convergence_episode==0:
              convergence_episode=episode
            net.opt.zero_grad() #optimization (use TRPO instead?)
            loss.backward() #policy net weights updated based on loss function
            net.opt.step()
  
        step +=1
        if done:
          break
  
  weight_socioecon=weight_socioeconIN #%
  weight_infected=weight_infectedIN
  weight_time=weight_timeIN
  weights=np.array([weight_socioecon,weight_infected,weight_time])
  Vaccine = VaccineIN
  preTrained = preTrainedIN
  onlyExploit = onlyExploitIN
  i1_u_0 = 0.098765
  i1_m_0 = 0.099999
  i1_o_0 = 0.03651

  x_u = (1.-i1_u_0,i1_u_0,0,0,0,0)
  x_m = (1.-i1_m_0,i1_m_0,0,0,0,0)
  x_o = (1.-i1_o_0,i1_o_0,0,0,0,0)
  initial_state=np.concatenate((x_u,x_m),axis=None)
  initial_state=np.concatenate((initial_state,x_o),axis=None)

  initial_vaccination=(0.8e-8,1.4e-4,1e-5)

  env = Environment(perc,weights,Vaccine,initial_state,initial_vaccination) #define environment object
  net = DeepQNetwork() #define policy net
  net2 = DeepQNetwork()  #define target net

  if preTrained:
    state_dict=torch.load(path) #load weight file from drive
    net.load_state_dict(state_dict) #upload weights in policy net 
    net2.load_state_dict(state_dict) #upload weights in target net

  run_deepQ(env, net, net2, onlyExploit)

  torch.save(net.state_dict(),path) #save weights file in Drive

dqn(0,100,50,True,True,True)